# Soft Actor-Critic (SAC) â€” LunarLanderContinuous-v3 (PyTorch from Scratch)

This repository contains a from-scratch implementation of the **Soft Actor-Critic (SAC)** algorithm in PyTorch.  
SAC is an off-policy actor-critic method that maximizes expected return **while also maximizing entropy**, encouraging more exploratory and robust policies.

---

## Key Features
- Fully implemented from scratch in PyTorch  
- **Entropy-regularized objective** for stable exploration  
- **Twin Q-networks** to reduce overestimation bias  
- **Target value smoothing** for stability  
- **Replay Buffer** for efficient experience reuse  
- Modular implementation with training, evaluation, and recording scripts  

---

## Training Results
The agent is trained on **LunarLanderContinuous-v3** (Gymnasium), achieving stable learning and smooth landings.  

ðŸ“ˆ **Reward Curve:**  
![Training Rewards](./reward-result.png)

ðŸŽ¥ **Demo Video:**  
[sac_lander_demo-episode-0.mp4](./sac_lander_demo-episode-0.mp4)

---

## File Structure
- `SAC.ipynb` â€” Jupyter/Colab notebook for experiments  
- `sac-skeleton.py` â€” Core SAC agent (actor, twin critics, entropy tuning)  
- `train-record-setup.py` â€” Training and logging setup  
- `eval.py` â€” Evaluation of trained agent  
- `record-demo.py` â€” Script to generate MP4 demo  
- `networks.py` â€” Actor and critic neural networks  
- `replaybuffer.py` â€” Replay buffer implementation  
- `utils.py` â€” Helper functions  
- `reward-result.png` â€” Training reward curve  
- `sac_lander_demo-episode-0.mp4` â€” Demo video of trained agent  

---

## Quickstart
```bash
# Clone repository
git clone https://github.com/DHAANUS/SAC-LunarLander-from-scratch.git
cd SAC-LunarLander-from-scratch

# Install dependencies
pip install torch gymnasium[box2d] imageio opencv-python

# Train the agent
python train-record-setup.py

# Evaluate and record demo
python record-demo.py
